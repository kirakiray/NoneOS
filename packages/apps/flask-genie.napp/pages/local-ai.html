<template page>
  <link rel="stylesheet" href="../others/public.css" />
  <l-m src="../comps/local-ollama-tags.html"></l-m>
  <style>
    :host {
      display: block;
      padding: 16px;
    }
  </style>
  <h5>本机AI</h5>
  <p>
    依托本地系统安装的Ollama接口，让NoneOS内的应用能够直接调用AI能力，解锁更多丰富的AI功能。
  </p>
  <h5>本地已下载的模型</h5>
  <local-ollama-tags></local-ollama-tags>

  <script>
    export const parent = "./layout.html";

    export default async ({ load }) => {
      return {
        data: {
          val: "123",
        },
        async attached() {
          this.emit("update-tab", {
            data: "local-ai",
            composed: true,
          });
        },
      };
    };

    // let text = "";

    // 使用示例
    // askOllamaStream("/no_think 你是谁？", "qwen3:4b", (chunk) => {
    //   text += chunk;
    //   console.clear();
    //   console.log(text); // 实时输出每个片段
    // });
  </script>
</template>
