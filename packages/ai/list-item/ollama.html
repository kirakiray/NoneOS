<template page>
  <style>
    :host {
      display: block;
    }

    n-local-icon {
      display: block;
      margin-right: 4px;
      color: var(--md-sys-color-primary);
      font-size: 18px;
    }
  </style>
  <p-list-item collapse-childs="close" button="suffix">
    <img
      src="../sources/ollama.svg"
      width="18"
      height="18"
      slot="prefix"
      style="margin-right: 4px; background-color: #fff; border-radius: 4px"
    />

    <div style="display: flex; align-items: center">
      Ollama
      <o-if :value="item.url.includes('localhost')">
        <localized-content>
          <span lang="cn"> (本机AI) </span>
          <span lang="en"> (Local AI) </span>
          <span lang="ja"> (ローカルのAI) </span>
        </localized-content>
      </o-if>
    </div>
    <i toggle-collapse triangle slot="suffix"></i>
    <p-list slot="childs" style="--ladder-left: 16px">
      <p-list-item>
        <o-if :value="!!item">
          <div style="display: flex; align-items: center">
            <p-input
              size="small"
              sync:value="item.url"
              style="width: 260px; margin-left: 4px"
            >
              <label>URL</label>
            </p-input>
          </div>
        </o-if>
      </p-list-item>
      <o-if :value="error">
        <localized-content>
          <div lang="cn">
            无法连接到本机的 Ollama Server，请按以下步骤检查配置：<br />
            1. 确认 Ollama 已正确安装并正在运行。<br />
            2. 确认项目配置的端口号与 Ollama Server 的端口号一致（默认:
            11434）。<br />
            3. 确认在 Ollama 配置中启用跨域访问。<br />
          </div>
          <div lang="en">
            Unable to connect to the local Ollama Server, please check the
            configuration steps below:<br />
            1. Confirm that Ollama is installed and running correctly.<br />
            2. Confirm that the port number in the project configuration matches
            the port number of the Ollama Server (default: 11434).<br />
            3. Confirm that cross-origin access is enabled in the Ollama
            configuration.<br />
          </div>
          <div lang="ja">
            ローカルのOllama
            Serverに接続できません。以下の手順を確認してください：<br />
            1. Ollamaが正しくインストールされていることを確認してください。<br />
            2. プロジェクトの設定でOllama
            Serverのポート番号が正しいことを確認してください（デフォルト:
            11434）。<br />
            3.
            Ollamaの設定で跨域アクセスが有効になっていることを確認してください。<br />
          </div>
        </localized-content>
      </o-if>
      <o-else>
        <p-list-item collapse-childs="open" button="suffix">
          <n-local-icon name="ai-model" slot="prefix"></n-local-icon>
          <localized-content>
            <span lang="cn"> 请选择默认AI模型：</span>
            <span lang="en"> Please select the default AI model:</span>
            <span lang="ja"> デフォルトのAIモデルを選択してください：</span>
          </localized-content>
          <i toggle-collapse triangle slot="suffix"></i>
          <p-list slot="childs">
            <o-fill :value="list" fill-key="name">
              <p-list-item
                button
                on:click="$host.useIt($data)"
                attr:active-item="$host.item.model === $data.name"
              >
                <o-if :value="$data.name.includes('qwen')" slot="prefix">
                  <n-local-icon name="qwen"></n-local-icon>
                </o-if>
                <o-else slot="prefix">
                  <n-local-icon name="ai-chip"></n-local-icon>
                </o-else>
                {{$data.name}} - ({{$data.size}})
                <o-if :value="$host.item.model === $data.name" slot="suffix">
                  <n-local-icon
                    name="tick-circle"
                    style="color: var(--md-sys-color-success)"
                  ></n-local-icon>
                </o-if>
              </p-list-item>
            </o-fill>
          </p-list>
        </p-list-item>
        <p-list-item>
          <div>
            {{fullResponse}}
            <p-button
              on:click="testModel"
              size="mini"
              attr:disabled="responsing"
              variant="outlined"
              style="margin: 4px"
            >
              <localized-content>
                <span lang="cn"> 你是？</span>
                <span lang="en"> Who are you?</span>
                <span lang="ja"> あなたは？</span>
              </localized-content>
            </p-button>
          </div>
        </p-list-item>
      </o-else>
      <p-list-item>
        <div style="display: flex">
          <p-button
            color="error"
            size="small"
            variant="outlined"
            on:click="emit('click-delete')"
          >
            <localized-content>
              <span lang="cn"> 删除该AI</span>
              <span lang="en"> Delete this AI</span>
              <span lang="ja"> このAIを削除</span>
            </localized-content>
          </p-button>
        </div>
      </p-list-item>
    </p-list>
  </p-list-item>

  <script>
    export default async ({ load }) => {
      const { chat, getModels } = await load("../ollama.js");

      const { getLocalized } = await load("/packages/i18n/localized-object.js");

      return {
        data: {
          //   item: {
          //   url: ""
          // },
          error: "",
          list: [], // 模型列表
          fullResponse: "",
          responsing: false,
        },
        watch: {
          item(item) {
            if (item && item.url) {
              this.initModels();
            }
          },
        },
        proto: {
          async testModel() {
            this.responsing = true;

            const prompt = await getLocalized({
              cn: "你是谁？回答的内容简短一点",
              en: "Who are you? Just answer",
              ja: "あなたは？直接回答してください。",
            });

            try {
              const serverUrl = this.item.url;

              const model = this.item.model;

              const result = await chat({
                serverUrl,
                model,
                messages: [{ role: "user", content: prompt }],
                onChunk: (e) => {
                  const { fullResponse } = e;
                  if (this.item.model === model) {
                    this.fullResponse = fullResponse;
                  }
                },
              });

              if (this.item.model === model) {
                this.fullResponse = result;
              }

              this.fullResponse = result;
            } catch (err) {
              this.fullResponse = await getLocalized({
                cn: "测试模型失败",
                en: "Test model failed",
                ja: "モデルのテストに失敗しました",
              });
            } finally {
              this.responsing = false;
            }
          },
          useIt(data) {
            this.item.model = data.name;
            this.testModel();
          },
          async initModels() {
            try {
              const serverUrl = this.item.url;
              const list = await getModels(serverUrl);

              // 格式化模型大小显示
              list.forEach((model) => {
                if (model.size) {
                  const sizeInGB = (model.size / (1024 * 1024 * 1024)).toFixed(
                    2
                  );
                  model.size = `${sizeInGB} GB`;
                }
              });

              list.sort((a, b) => {
                if (a.name < b.name) {
                  return -1;
                }
                if (a.name > b.name) {
                  return 1;
                }
                return 0;
              });

              this.list = list;
              this.error = "";
            } catch (err) {
              this.error = err.message;
              this.list = [];
            }
          },
        },
        attached() {
          // 设置默认端口
        },
        detached() {
          this.item = {};
        },
      };
    };
  </script>
</template>
